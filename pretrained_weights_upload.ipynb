{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "\n",
    "\n",
    "# Function to upload and load the model\n",
    "def load_model(model_path: str):\n",
    "    state_dict = torch.load(model_path, map_location=\"cpu\", weights_only=True)\n",
    "    return state_dict\n",
    "\n",
    "\n",
    "def load_config_trained(config_path: str):\n",
    "    \"\"\"\n",
    "    loads the json config file for the trained model\n",
    "    \"\"\"\n",
    "    with open(config_path, \"r\") as f:\n",
    "        config = json.load(f)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading pre-trained weigths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deit.embeddings.cls_token: torch.Size([1, 1, 768])\n",
      "deit.embeddings.position_embeddings: torch.Size([1, 197, 768])\n",
      "deit.embeddings.patch_embeddings.projection.weight: torch.Size([768, 3, 16, 16])\n",
      "deit.embeddings.patch_embeddings.projection.bias: torch.Size([768])\n",
      "deit.encoder.layer.0.attention.attention.query.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.0.attention.attention.query.bias: torch.Size([768])\n",
      "deit.encoder.layer.0.attention.attention.key.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.0.attention.attention.key.bias: torch.Size([768])\n",
      "deit.encoder.layer.0.attention.attention.value.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.0.attention.attention.value.bias: torch.Size([768])\n",
      "deit.encoder.layer.0.attention.output.dense.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.0.attention.output.dense.bias: torch.Size([768])\n",
      "deit.encoder.layer.0.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "deit.encoder.layer.0.intermediate.dense.bias: torch.Size([3072])\n",
      "deit.encoder.layer.0.output.dense.weight: torch.Size([768, 3072])\n",
      "deit.encoder.layer.0.output.dense.bias: torch.Size([768])\n",
      "deit.encoder.layer.0.layernorm_before.weight: torch.Size([768])\n",
      "deit.encoder.layer.0.layernorm_before.bias: torch.Size([768])\n",
      "deit.encoder.layer.0.layernorm_after.weight: torch.Size([768])\n",
      "deit.encoder.layer.0.layernorm_after.bias: torch.Size([768])\n",
      "deit.encoder.layer.1.attention.attention.query.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.1.attention.attention.query.bias: torch.Size([768])\n",
      "deit.encoder.layer.1.attention.attention.key.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.1.attention.attention.key.bias: torch.Size([768])\n",
      "deit.encoder.layer.1.attention.attention.value.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.1.attention.attention.value.bias: torch.Size([768])\n",
      "deit.encoder.layer.1.attention.output.dense.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.1.attention.output.dense.bias: torch.Size([768])\n",
      "deit.encoder.layer.1.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "deit.encoder.layer.1.intermediate.dense.bias: torch.Size([3072])\n",
      "deit.encoder.layer.1.output.dense.weight: torch.Size([768, 3072])\n",
      "deit.encoder.layer.1.output.dense.bias: torch.Size([768])\n",
      "deit.encoder.layer.1.layernorm_before.weight: torch.Size([768])\n",
      "deit.encoder.layer.1.layernorm_before.bias: torch.Size([768])\n",
      "deit.encoder.layer.1.layernorm_after.weight: torch.Size([768])\n",
      "deit.encoder.layer.1.layernorm_after.bias: torch.Size([768])\n",
      "deit.encoder.layer.2.attention.attention.query.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.2.attention.attention.query.bias: torch.Size([768])\n",
      "deit.encoder.layer.2.attention.attention.key.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.2.attention.attention.key.bias: torch.Size([768])\n",
      "deit.encoder.layer.2.attention.attention.value.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.2.attention.attention.value.bias: torch.Size([768])\n",
      "deit.encoder.layer.2.attention.output.dense.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.2.attention.output.dense.bias: torch.Size([768])\n",
      "deit.encoder.layer.2.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "deit.encoder.layer.2.intermediate.dense.bias: torch.Size([3072])\n",
      "deit.encoder.layer.2.output.dense.weight: torch.Size([768, 3072])\n",
      "deit.encoder.layer.2.output.dense.bias: torch.Size([768])\n",
      "deit.encoder.layer.2.layernorm_before.weight: torch.Size([768])\n",
      "deit.encoder.layer.2.layernorm_before.bias: torch.Size([768])\n",
      "deit.encoder.layer.2.layernorm_after.weight: torch.Size([768])\n",
      "deit.encoder.layer.2.layernorm_after.bias: torch.Size([768])\n",
      "deit.encoder.layer.3.attention.attention.query.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.3.attention.attention.query.bias: torch.Size([768])\n",
      "deit.encoder.layer.3.attention.attention.key.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.3.attention.attention.key.bias: torch.Size([768])\n",
      "deit.encoder.layer.3.attention.attention.value.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.3.attention.attention.value.bias: torch.Size([768])\n",
      "deit.encoder.layer.3.attention.output.dense.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.3.attention.output.dense.bias: torch.Size([768])\n",
      "deit.encoder.layer.3.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "deit.encoder.layer.3.intermediate.dense.bias: torch.Size([3072])\n",
      "deit.encoder.layer.3.output.dense.weight: torch.Size([768, 3072])\n",
      "deit.encoder.layer.3.output.dense.bias: torch.Size([768])\n",
      "deit.encoder.layer.3.layernorm_before.weight: torch.Size([768])\n",
      "deit.encoder.layer.3.layernorm_before.bias: torch.Size([768])\n",
      "deit.encoder.layer.3.layernorm_after.weight: torch.Size([768])\n",
      "deit.encoder.layer.3.layernorm_after.bias: torch.Size([768])\n",
      "deit.encoder.layer.4.attention.attention.query.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.4.attention.attention.query.bias: torch.Size([768])\n",
      "deit.encoder.layer.4.attention.attention.key.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.4.attention.attention.key.bias: torch.Size([768])\n",
      "deit.encoder.layer.4.attention.attention.value.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.4.attention.attention.value.bias: torch.Size([768])\n",
      "deit.encoder.layer.4.attention.output.dense.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.4.attention.output.dense.bias: torch.Size([768])\n",
      "deit.encoder.layer.4.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "deit.encoder.layer.4.intermediate.dense.bias: torch.Size([3072])\n",
      "deit.encoder.layer.4.output.dense.weight: torch.Size([768, 3072])\n",
      "deit.encoder.layer.4.output.dense.bias: torch.Size([768])\n",
      "deit.encoder.layer.4.layernorm_before.weight: torch.Size([768])\n",
      "deit.encoder.layer.4.layernorm_before.bias: torch.Size([768])\n",
      "deit.encoder.layer.4.layernorm_after.weight: torch.Size([768])\n",
      "deit.encoder.layer.4.layernorm_after.bias: torch.Size([768])\n",
      "deit.encoder.layer.5.attention.attention.query.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.5.attention.attention.query.bias: torch.Size([768])\n",
      "deit.encoder.layer.5.attention.attention.key.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.5.attention.attention.key.bias: torch.Size([768])\n",
      "deit.encoder.layer.5.attention.attention.value.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.5.attention.attention.value.bias: torch.Size([768])\n",
      "deit.encoder.layer.5.attention.output.dense.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.5.attention.output.dense.bias: torch.Size([768])\n",
      "deit.encoder.layer.5.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "deit.encoder.layer.5.intermediate.dense.bias: torch.Size([3072])\n",
      "deit.encoder.layer.5.output.dense.weight: torch.Size([768, 3072])\n",
      "deit.encoder.layer.5.output.dense.bias: torch.Size([768])\n",
      "deit.encoder.layer.5.layernorm_before.weight: torch.Size([768])\n",
      "deit.encoder.layer.5.layernorm_before.bias: torch.Size([768])\n",
      "deit.encoder.layer.5.layernorm_after.weight: torch.Size([768])\n",
      "deit.encoder.layer.5.layernorm_after.bias: torch.Size([768])\n",
      "deit.encoder.layer.6.attention.attention.query.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.6.attention.attention.query.bias: torch.Size([768])\n",
      "deit.encoder.layer.6.attention.attention.key.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.6.attention.attention.key.bias: torch.Size([768])\n",
      "deit.encoder.layer.6.attention.attention.value.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.6.attention.attention.value.bias: torch.Size([768])\n",
      "deit.encoder.layer.6.attention.output.dense.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.6.attention.output.dense.bias: torch.Size([768])\n",
      "deit.encoder.layer.6.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "deit.encoder.layer.6.intermediate.dense.bias: torch.Size([3072])\n",
      "deit.encoder.layer.6.output.dense.weight: torch.Size([768, 3072])\n",
      "deit.encoder.layer.6.output.dense.bias: torch.Size([768])\n",
      "deit.encoder.layer.6.layernorm_before.weight: torch.Size([768])\n",
      "deit.encoder.layer.6.layernorm_before.bias: torch.Size([768])\n",
      "deit.encoder.layer.6.layernorm_after.weight: torch.Size([768])\n",
      "deit.encoder.layer.6.layernorm_after.bias: torch.Size([768])\n",
      "deit.encoder.layer.7.attention.attention.query.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.7.attention.attention.query.bias: torch.Size([768])\n",
      "deit.encoder.layer.7.attention.attention.key.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.7.attention.attention.key.bias: torch.Size([768])\n",
      "deit.encoder.layer.7.attention.attention.value.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.7.attention.attention.value.bias: torch.Size([768])\n",
      "deit.encoder.layer.7.attention.output.dense.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.7.attention.output.dense.bias: torch.Size([768])\n",
      "deit.encoder.layer.7.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "deit.encoder.layer.7.intermediate.dense.bias: torch.Size([3072])\n",
      "deit.encoder.layer.7.output.dense.weight: torch.Size([768, 3072])\n",
      "deit.encoder.layer.7.output.dense.bias: torch.Size([768])\n",
      "deit.encoder.layer.7.layernorm_before.weight: torch.Size([768])\n",
      "deit.encoder.layer.7.layernorm_before.bias: torch.Size([768])\n",
      "deit.encoder.layer.7.layernorm_after.weight: torch.Size([768])\n",
      "deit.encoder.layer.7.layernorm_after.bias: torch.Size([768])\n",
      "deit.encoder.layer.8.attention.attention.query.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.8.attention.attention.query.bias: torch.Size([768])\n",
      "deit.encoder.layer.8.attention.attention.key.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.8.attention.attention.key.bias: torch.Size([768])\n",
      "deit.encoder.layer.8.attention.attention.value.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.8.attention.attention.value.bias: torch.Size([768])\n",
      "deit.encoder.layer.8.attention.output.dense.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.8.attention.output.dense.bias: torch.Size([768])\n",
      "deit.encoder.layer.8.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "deit.encoder.layer.8.intermediate.dense.bias: torch.Size([3072])\n",
      "deit.encoder.layer.8.output.dense.weight: torch.Size([768, 3072])\n",
      "deit.encoder.layer.8.output.dense.bias: torch.Size([768])\n",
      "deit.encoder.layer.8.layernorm_before.weight: torch.Size([768])\n",
      "deit.encoder.layer.8.layernorm_before.bias: torch.Size([768])\n",
      "deit.encoder.layer.8.layernorm_after.weight: torch.Size([768])\n",
      "deit.encoder.layer.8.layernorm_after.bias: torch.Size([768])\n",
      "deit.encoder.layer.9.attention.attention.query.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.9.attention.attention.query.bias: torch.Size([768])\n",
      "deit.encoder.layer.9.attention.attention.key.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.9.attention.attention.key.bias: torch.Size([768])\n",
      "deit.encoder.layer.9.attention.attention.value.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.9.attention.attention.value.bias: torch.Size([768])\n",
      "deit.encoder.layer.9.attention.output.dense.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.9.attention.output.dense.bias: torch.Size([768])\n",
      "deit.encoder.layer.9.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "deit.encoder.layer.9.intermediate.dense.bias: torch.Size([3072])\n",
      "deit.encoder.layer.9.output.dense.weight: torch.Size([768, 3072])\n",
      "deit.encoder.layer.9.output.dense.bias: torch.Size([768])\n",
      "deit.encoder.layer.9.layernorm_before.weight: torch.Size([768])\n",
      "deit.encoder.layer.9.layernorm_before.bias: torch.Size([768])\n",
      "deit.encoder.layer.9.layernorm_after.weight: torch.Size([768])\n",
      "deit.encoder.layer.9.layernorm_after.bias: torch.Size([768])\n",
      "deit.encoder.layer.10.attention.attention.query.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.10.attention.attention.query.bias: torch.Size([768])\n",
      "deit.encoder.layer.10.attention.attention.key.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.10.attention.attention.key.bias: torch.Size([768])\n",
      "deit.encoder.layer.10.attention.attention.value.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.10.attention.attention.value.bias: torch.Size([768])\n",
      "deit.encoder.layer.10.attention.output.dense.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.10.attention.output.dense.bias: torch.Size([768])\n",
      "deit.encoder.layer.10.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "deit.encoder.layer.10.intermediate.dense.bias: torch.Size([3072])\n",
      "deit.encoder.layer.10.output.dense.weight: torch.Size([768, 3072])\n",
      "deit.encoder.layer.10.output.dense.bias: torch.Size([768])\n",
      "deit.encoder.layer.10.layernorm_before.weight: torch.Size([768])\n",
      "deit.encoder.layer.10.layernorm_before.bias: torch.Size([768])\n",
      "deit.encoder.layer.10.layernorm_after.weight: torch.Size([768])\n",
      "deit.encoder.layer.10.layernorm_after.bias: torch.Size([768])\n",
      "deit.encoder.layer.11.attention.attention.query.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.11.attention.attention.query.bias: torch.Size([768])\n",
      "deit.encoder.layer.11.attention.attention.key.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.11.attention.attention.key.bias: torch.Size([768])\n",
      "deit.encoder.layer.11.attention.attention.value.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.11.attention.attention.value.bias: torch.Size([768])\n",
      "deit.encoder.layer.11.attention.output.dense.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.11.attention.output.dense.bias: torch.Size([768])\n",
      "deit.encoder.layer.11.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "deit.encoder.layer.11.intermediate.dense.bias: torch.Size([3072])\n",
      "deit.encoder.layer.11.output.dense.weight: torch.Size([768, 3072])\n",
      "deit.encoder.layer.11.output.dense.bias: torch.Size([768])\n",
      "deit.encoder.layer.11.layernorm_before.weight: torch.Size([768])\n",
      "deit.encoder.layer.11.layernorm_before.bias: torch.Size([768])\n",
      "deit.encoder.layer.11.layernorm_after.weight: torch.Size([768])\n",
      "deit.encoder.layer.11.layernorm_after.bias: torch.Size([768])\n",
      "deit.encoder.highway.0.mlp.conv1.0.weight: torch.Size([768, 768, 1, 1])\n",
      "deit.encoder.highway.0.mlp.conv1.0.bias: torch.Size([768])\n",
      "deit.encoder.highway.0.mlp.conv1.2.weight: torch.Size([768])\n",
      "deit.encoder.highway.0.mlp.conv1.2.bias: torch.Size([768])\n",
      "deit.encoder.highway.0.mlp.conv1.2.running_mean: torch.Size([768])\n",
      "deit.encoder.highway.0.mlp.conv1.2.running_var: torch.Size([768])\n",
      "deit.encoder.highway.0.mlp.conv1.2.num_batches_tracked: torch.Size([])\n",
      "deit.encoder.highway.0.mlp.proj.weight: torch.Size([768, 1, 3, 3])\n",
      "deit.encoder.highway.0.mlp.proj.bias: torch.Size([768])\n",
      "deit.encoder.highway.0.mlp.proj_bn.weight: torch.Size([768])\n",
      "deit.encoder.highway.0.mlp.proj_bn.bias: torch.Size([768])\n",
      "deit.encoder.highway.0.mlp.proj_bn.running_mean: torch.Size([768])\n",
      "deit.encoder.highway.0.mlp.proj_bn.running_var: torch.Size([768])\n",
      "deit.encoder.highway.0.mlp.proj_bn.num_batches_tracked: torch.Size([])\n",
      "deit.encoder.highway.0.mlp.conv2.0.weight: torch.Size([768, 768, 1, 1])\n",
      "deit.encoder.highway.0.mlp.conv2.0.bias: torch.Size([768])\n",
      "deit.encoder.highway.0.mlp.conv2.1.weight: torch.Size([768])\n",
      "deit.encoder.highway.0.mlp.conv2.1.bias: torch.Size([768])\n",
      "deit.encoder.highway.0.mlp.conv2.1.running_mean: torch.Size([768])\n",
      "deit.encoder.highway.0.mlp.conv2.1.running_var: torch.Size([768])\n",
      "deit.encoder.highway.0.mlp.conv2.1.num_batches_tracked: torch.Size([])\n",
      "deit.encoder.highway.0.classifier.weight: torch.Size([100, 768])\n",
      "deit.encoder.highway.0.classifier.bias: torch.Size([100])\n",
      "deit.encoder.highway.1.mlp.conv1.0.weight: torch.Size([768, 768, 1, 1])\n",
      "deit.encoder.highway.1.mlp.conv1.0.bias: torch.Size([768])\n",
      "deit.encoder.highway.1.mlp.conv1.2.weight: torch.Size([768])\n",
      "deit.encoder.highway.1.mlp.conv1.2.bias: torch.Size([768])\n",
      "deit.encoder.highway.1.mlp.conv1.2.running_mean: torch.Size([768])\n",
      "deit.encoder.highway.1.mlp.conv1.2.running_var: torch.Size([768])\n",
      "deit.encoder.highway.1.mlp.conv1.2.num_batches_tracked: torch.Size([])\n",
      "deit.encoder.highway.1.mlp.proj.weight: torch.Size([768, 1, 3, 3])\n",
      "deit.encoder.highway.1.mlp.proj.bias: torch.Size([768])\n",
      "deit.encoder.highway.1.mlp.proj_bn.weight: torch.Size([768])\n",
      "deit.encoder.highway.1.mlp.proj_bn.bias: torch.Size([768])\n",
      "deit.encoder.highway.1.mlp.proj_bn.running_mean: torch.Size([768])\n",
      "deit.encoder.highway.1.mlp.proj_bn.running_var: torch.Size([768])\n",
      "deit.encoder.highway.1.mlp.proj_bn.num_batches_tracked: torch.Size([])\n",
      "deit.encoder.highway.1.mlp.conv2.0.weight: torch.Size([768, 768, 1, 1])\n",
      "deit.encoder.highway.1.mlp.conv2.0.bias: torch.Size([768])\n",
      "deit.encoder.highway.1.mlp.conv2.1.weight: torch.Size([768])\n",
      "deit.encoder.highway.1.mlp.conv2.1.bias: torch.Size([768])\n",
      "deit.encoder.highway.1.mlp.conv2.1.running_mean: torch.Size([768])\n",
      "deit.encoder.highway.1.mlp.conv2.1.running_var: torch.Size([768])\n",
      "deit.encoder.highway.1.mlp.conv2.1.num_batches_tracked: torch.Size([])\n",
      "deit.encoder.highway.1.classifier.weight: torch.Size([100, 768])\n",
      "deit.encoder.highway.1.classifier.bias: torch.Size([100])\n",
      "deit.encoder.highway.2.mlp.conv1.0.weight: torch.Size([768, 768, 1, 1])\n",
      "deit.encoder.highway.2.mlp.conv1.0.bias: torch.Size([768])\n",
      "deit.encoder.highway.2.mlp.conv1.2.weight: torch.Size([768])\n",
      "deit.encoder.highway.2.mlp.conv1.2.bias: torch.Size([768])\n",
      "deit.encoder.highway.2.mlp.conv1.2.running_mean: torch.Size([768])\n",
      "deit.encoder.highway.2.mlp.conv1.2.running_var: torch.Size([768])\n",
      "deit.encoder.highway.2.mlp.conv1.2.num_batches_tracked: torch.Size([])\n",
      "deit.encoder.highway.2.mlp.conv2.0.weight: torch.Size([768, 768, 1, 1])\n",
      "deit.encoder.highway.2.mlp.conv2.0.bias: torch.Size([768])\n",
      "deit.encoder.highway.2.mlp.conv2.1.weight: torch.Size([768])\n",
      "deit.encoder.highway.2.mlp.conv2.1.bias: torch.Size([768])\n",
      "deit.encoder.highway.2.mlp.conv2.1.running_mean: torch.Size([768])\n",
      "deit.encoder.highway.2.mlp.conv2.1.running_var: torch.Size([768])\n",
      "deit.encoder.highway.2.mlp.conv2.1.num_batches_tracked: torch.Size([])\n",
      "deit.encoder.highway.2.classifier.weight: torch.Size([100, 768])\n",
      "deit.encoder.highway.2.classifier.bias: torch.Size([100])\n",
      "deit.encoder.highway.3.mlp.conv1.0.weight: torch.Size([768, 768, 1, 1])\n",
      "deit.encoder.highway.3.mlp.conv1.0.bias: torch.Size([768])\n",
      "deit.encoder.highway.3.mlp.conv1.2.weight: torch.Size([768])\n",
      "deit.encoder.highway.3.mlp.conv1.2.bias: torch.Size([768])\n",
      "deit.encoder.highway.3.mlp.conv1.2.running_mean: torch.Size([768])\n",
      "deit.encoder.highway.3.mlp.conv1.2.running_var: torch.Size([768])\n",
      "deit.encoder.highway.3.mlp.conv1.2.num_batches_tracked: torch.Size([])\n",
      "deit.encoder.highway.3.mlp.conv2.0.weight: torch.Size([768, 768, 1, 1])\n",
      "deit.encoder.highway.3.mlp.conv2.0.bias: torch.Size([768])\n",
      "deit.encoder.highway.3.mlp.conv2.1.weight: torch.Size([768])\n",
      "deit.encoder.highway.3.mlp.conv2.1.bias: torch.Size([768])\n",
      "deit.encoder.highway.3.mlp.conv2.1.running_mean: torch.Size([768])\n",
      "deit.encoder.highway.3.mlp.conv2.1.running_var: torch.Size([768])\n",
      "deit.encoder.highway.3.mlp.conv2.1.num_batches_tracked: torch.Size([])\n",
      "deit.encoder.highway.3.classifier.weight: torch.Size([100, 768])\n",
      "deit.encoder.highway.3.classifier.bias: torch.Size([100])\n",
      "deit.encoder.highway.4.mlp.qkv.weight: torch.Size([2304, 768])\n",
      "deit.encoder.highway.4.mlp.proj.weight: torch.Size([768, 768])\n",
      "deit.encoder.highway.4.mlp.proj.bias: torch.Size([768])\n",
      "deit.encoder.highway.4.classifier.weight: torch.Size([100, 768])\n",
      "deit.encoder.highway.4.classifier.bias: torch.Size([100])\n",
      "deit.encoder.highway.5.mlp.qkv.weight: torch.Size([2304, 768])\n",
      "deit.encoder.highway.5.mlp.proj.weight: torch.Size([768, 768])\n",
      "deit.encoder.highway.5.mlp.proj.bias: torch.Size([768])\n",
      "deit.encoder.highway.5.classifier.weight: torch.Size([100, 768])\n",
      "deit.encoder.highway.5.classifier.bias: torch.Size([100])\n",
      "deit.encoder.highway.6.mlp.qkv.weight: torch.Size([2304, 768])\n",
      "deit.encoder.highway.6.mlp.proj.weight: torch.Size([768, 768])\n",
      "deit.encoder.highway.6.mlp.proj.bias: torch.Size([768])\n",
      "deit.encoder.highway.6.classifier.weight: torch.Size([100, 768])\n",
      "deit.encoder.highway.6.classifier.bias: torch.Size([100])\n",
      "deit.encoder.highway.7.mlp.qkv.weight: torch.Size([2304, 768])\n",
      "deit.encoder.highway.7.mlp.proj.weight: torch.Size([768, 768])\n",
      "deit.encoder.highway.7.mlp.proj.bias: torch.Size([768])\n",
      "deit.encoder.highway.7.classifier.weight: torch.Size([100, 768])\n",
      "deit.encoder.highway.7.classifier.bias: torch.Size([100])\n",
      "deit.layernorm.weight: torch.Size([768])\n",
      "deit.layernorm.bias: torch.Size([768])\n",
      "deit.pooler.dense.weight: torch.Size([768, 768])\n",
      "deit.pooler.dense.bias: torch.Size([768])\n",
      "classifier.weight: torch.Size([100, 768])\n",
      "classifier.bias: torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "# Load the model and config\n",
    "saved_model_state_dict = load_model(\n",
    "    \"/home/iony/DTU/f24/thesis/code/lgvit/LGViT-ViT-Cifar100/pytorch_model.bin\"\n",
    ")\n",
    "config_pretrained = load_config_trained(\n",
    "    \"/home/iony/DTU/f24/thesis/code/lgvit/LGViT-ViT-Cifar100/config.json\"\n",
    ")\n",
    "# Display the model structure\n",
    "str_match = \"\"  # \"highway\" || \"transformer\"\n",
    "for key, value in saved_model_state_dict.items():\n",
    "    if str_match in key:\n",
    "        print(f\"{key}: {value.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LGVIT -> EEVIT mapping\n",
    "\n",
    "### EEVIT components\n",
    "* patch_embedding: PatchEmbeddings ✅\n",
    "* transformer: TransformerEnconder\n",
    "  * attention_layers: List\\[Attention\\]\n",
    "    * norm: LayerNorm\n",
    "    * W_QKV: Linear \n",
    "  * norm: LayerNorm\n",
    "* ToLatent\n",
    "* LastClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch Embeddings\n",
    "lgvit_map = {\n",
    "    \"patch_embedding.pos_embedding\": \"deit.embeddings.position_embeddings\",\n",
    "    \"patch_embedding.cls_token\": \"deit.embeddings.cls_token\",\n",
    "    \"patch_embedding.projection.weight\": \"deit.embeddings.patch_embeddings.projection.weight\",\n",
    "    \"patch_embedding.projection.bias\": \"deit.embeddings.patch_embeddings.projection.bias\",\n",
    "}\n",
    "\n",
    "# Transformer layers\n",
    "lgvit_map[\"transformer.norm_post_layers.weight\"] = \"deit.layernorm.weight\"\n",
    "lgvit_map[\"transformer.norm_post_layers.bias\"] = \"deit.layernorm.bias\"\n",
    "\n",
    "for i in range(config_pretrained[\"num_hidden_layers\"]):\n",
    "    lgvit_map[f\"transformer.layers.{i}.norm_1.weight\"] = (\n",
    "        f\"deit.encoder.layer.{i}.layernorm_before.weight\"\n",
    "    )\n",
    "    lgvit_map[f\"transformer.layers.{i}.norm_1.bias\"] = (\n",
    "        f\"deit.encoder.layer.{i}.layernorm_before.bias\"\n",
    "    )\n",
    "\n",
    "    lgvit_map[f\"transformer.layers.{i}.W_QKV.weight\"] = [\n",
    "        f\"deit.encoder.layer.{i}.attention.attention.query.weight\",\n",
    "        f\"deit.encoder.layer.{i}.attention.attention.key.weight\",\n",
    "        f\"deit.encoder.layer.{i}.attention.attention.value.weight\",\n",
    "    ]\n",
    "\n",
    "    lgvit_map[f\"transformer.layers.{i}.W_QKV.bias\"] = [\n",
    "        f\"deit.encoder.layer.{i}.attention.attention.query.bias\",\n",
    "        f\"deit.encoder.layer.{i}.attention.attention.key.bias\",\n",
    "        f\"deit.encoder.layer.{i}.attention.attention.value.bias\",\n",
    "    ]\n",
    "\n",
    "    lgvit_map[f\"transformer.layers.{i}.attention_output.0.weight\"] = (\n",
    "        f\"deit.encoder.layer.{i}.attention.output.dense.weight\"\n",
    "    )\n",
    "    lgvit_map[f\"transformer.layers.{i}.attention_output.0.bias\"] = (\n",
    "        f\"deit.encoder.layer.{i}.attention.output.dense.bias\"\n",
    "    )\n",
    "\n",
    "    lgvit_map[f\"transformer.layers.{i}.norm_mlp.norm.weight\"] = (\n",
    "        f\"deit.encoder.layer.{i}.layernorm_after.weight\"\n",
    "    )\n",
    "    lgvit_map[f\"transformer.layers.{i}.norm_mlp.norm.bias\"] = (\n",
    "        f\"deit.encoder.layer.{i}.layernorm_after.bias\"\n",
    "    )\n",
    "\n",
    "    lgvit_map[f\"transformer.layers.{i}.norm_mlp.mlp.0.weight\"] = (\n",
    "        f\"deit.encoder.layer.{i}.intermediate.dense.weight\"\n",
    "    )\n",
    "    lgvit_map[f\"transformer.layers.{i}.norm_mlp.mlp.0.bias\"] = (\n",
    "        f\"deit.encoder.layer.{i}.intermediate.dense.bias\"\n",
    "    )\n",
    "\n",
    "    lgvit_map[f\"transformer.layers.{i}.norm_mlp.mlp.3.weight\"] = (\n",
    "        f\"deit.encoder.layer.{i}.output.dense.weight\"\n",
    "    )\n",
    "    lgvit_map[f\"transformer.layers.{i}.norm_mlp.mlp.3.bias\"] = (\n",
    "        f\"deit.encoder.layer.{i}.output.dense.bias\"\n",
    "    )\n",
    "\n",
    "# last exit\n",
    "lgvit_map[\"last_exit.weight\"] = \"classifier.weight\"\n",
    "lgvit_map[\"last_exit.bias\"] = \"classifier.bias\"\n",
    "\n",
    "\n",
    "def make_values_dict(key2key_dict: dict):\n",
    "    values_dict = {}\n",
    "    for k, v in key2key_dict.items():\n",
    "        values_dict[k] = saved_model_state_dict[v]\n",
    "    return values_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deit.encoder.layer.0.layernorm_before.weight\n",
      "torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "k = lgvit_map[\"transformer.layers.0.norm_1.weight\"]\n",
    "print(k)\n",
    "v = saved_model_state_dict[k]\n",
    "print(v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch Embeddings\n",
    "lgvit_map = {\n",
    "    \"patch_embedding.pos_embedding\": saved_model_state_dict[\n",
    "        \"deit.embeddings.position_embeddings\"\n",
    "    ],\n",
    "    \"patch_embedding.cls_token\": saved_model_state_dict[\"deit.embeddings.cls_token\"],\n",
    "    \"patch_embedding.projection.weight\": saved_model_state_dict[\n",
    "        \"deit.embeddings.patch_embeddings.projection.weight\"\n",
    "    ],\n",
    "    \"patch_embedding.projection.bias\": saved_model_state_dict[\n",
    "        \"deit.embeddings.patch_embeddings.projection.bias\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Transformer layers\n",
    "lgvit_map[\"transformer.norm_post_layers.weight\"] = saved_model_state_dict[\n",
    "    \"deit.layernorm.weight\"\n",
    "]\n",
    "lgvit_map[\"transformer.norm_post_layers.bias\"] = saved_model_state_dict[\n",
    "    \"deit.layernorm.bias\"\n",
    "]\n",
    "\n",
    "for i in range(config_pretrained[\"num_hidden_layers\"]):\n",
    "    lgvit_map[f\"transformer.layers.{i}.norm_1.weight\"] = saved_model_state_dict[\n",
    "        f\"deit.encoder.layer.{i}.layernorm_before.weight\"\n",
    "    ]\n",
    "    lgvit_map[f\"transformer.layers.{i}.norm_1.bias\"] = saved_model_state_dict[\n",
    "        f\"deit.encoder.layer.{i}.layernorm_before.bias\"\n",
    "    ]\n",
    "\n",
    "    lgvit_map[f\"transformer.layers.{i}.W_QKV.weight\"] = torch.cat(\n",
    "        [\n",
    "            saved_model_state_dict[\n",
    "                f\"deit.encoder.layer.{i}.attention.attention.query.weight\"\n",
    "            ],\n",
    "            saved_model_state_dict[\n",
    "                f\"deit.encoder.layer.{i}.attention.attention.key.weight\"\n",
    "            ],\n",
    "            saved_model_state_dict[\n",
    "                f\"deit.encoder.layer.{i}.attention.attention.value.weight\"\n",
    "            ],\n",
    "        ],\n",
    "        dim=0,\n",
    "    )\n",
    "\n",
    "    lgvit_map[f\"transformer.layers.{i}.W_QKV.bias\"] = torch.cat(\n",
    "        [\n",
    "            saved_model_state_dict[\n",
    "                f\"deit.encoder.layer.{i}.attention.attention.query.bias\"\n",
    "            ],\n",
    "            saved_model_state_dict[\n",
    "                f\"deit.encoder.layer.{i}.attention.attention.key.bias\"\n",
    "            ],\n",
    "            saved_model_state_dict[\n",
    "                f\"deit.encoder.layer.{i}.attention.attention.value.bias\"\n",
    "            ],\n",
    "        ],\n",
    "        dim=0,\n",
    "    )\n",
    "\n",
    "    lgvit_map[f\"transformer.layers.{i}.attention_output.0.weight\"] = (\n",
    "        saved_model_state_dict[f\"deit.encoder.layer.{i}.attention.output.dense.weight\"]\n",
    "    )\n",
    "    lgvit_map[f\"transformer.layers.{i}.attention_output.0.bias\"] = (\n",
    "        saved_model_state_dict[f\"deit.encoder.layer.{i}.attention.output.dense.bias\"]\n",
    "    )\n",
    "\n",
    "    lgvit_map[f\"transformer.layers.{i}.norm_mlp.norm.weight\"] = saved_model_state_dict[\n",
    "        f\"deit.encoder.layer.{i}.layernorm_after.weight\"\n",
    "    ]\n",
    "    lgvit_map[f\"transformer.layers.{i}.norm_mlp.norm.bias\"] = saved_model_state_dict[\n",
    "        f\"deit.encoder.layer.{i}.layernorm_after.bias\"\n",
    "    ]\n",
    "\n",
    "    lgvit_map[f\"transformer.layers.{i}.norm_mlp.mlp.0.weight\"] = saved_model_state_dict[\n",
    "        f\"deit.encoder.layer.{i}.intermediate.dense.weight\"\n",
    "    ]\n",
    "    lgvit_map[f\"transformer.layers.{i}.norm_mlp.mlp.0.bias\"] = saved_model_state_dict[\n",
    "        f\"deit.encoder.layer.{i}.intermediate.dense.bias\"\n",
    "    ]\n",
    "\n",
    "    lgvit_map[f\"transformer.layers.{i}.norm_mlp.mlp.3.weight\"] = saved_model_state_dict[\n",
    "        f\"deit.encoder.layer.{i}.output.dense.weight\"\n",
    "    ]\n",
    "    lgvit_map[f\"transformer.layers.{i}.norm_mlp.mlp.3.bias\"] = saved_model_state_dict[\n",
    "        f\"deit.encoder.layer.{i}.output.dense.bias\"\n",
    "    ]\n",
    "\n",
    "# last exit\n",
    "lgvit_map[\"last_exit.weight\"] = saved_model_state_dict[\"classifier.weight\"]\n",
    "lgvit_map[\"last_exit.bias\"] = saved_model_state_dict[\"classifier.bias\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing EEVIT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [16:29:00.073][eevit.py]: Initializing Vit model...\n",
      "[INFO] [16:29:00.074][vit_classes.py]: Initializing PatchEmbeddings...\n",
      "[INFO] [16:29:00.086][vit_classes.py]: PatchEmbedding initialized with 197 patches (including the cls token)\n",
      "[INFO] [16:29:00.366][vit_classes.py]: Highway of type 'conv1_1({})' appended to location '4'\n",
      "[INFO] [16:29:00.465][vit_classes.py]: Highway of type 'conv1_1({})' appended to location '5'\n",
      "[INFO] [16:29:00.543][vit_classes.py]: Highway of type 'conv2_1({})' appended to location '6'\n",
      "[INFO] [16:29:00.619][vit_classes.py]: Highway of type 'conv2_1({})' appended to location '7'\n",
      "[INFO] [16:29:00.744][vit_classes.py]: Highway of type 'attention({'sr_ratio': 2})' appended to location '8'\n",
      "[INFO] [16:29:00.841][vit_classes.py]: Highway of type 'attention({'sr_ratio': 2})' appended to location '9'\n",
      "[INFO] [16:29:00.927][vit_classes.py]: Highway of type 'attention({'sr_ratio': 3})' appended to location '10'\n",
      "[INFO] [16:29:01.014][vit_classes.py]: Highway of type 'attention({'sr_ratio': 3})' appended to location '11'\n",
      "[INFO] [16:29:01.079][vit_classes.py]: TransformerEnconder initialized with 12 layers and 8 early exits\n",
      "[INFO] [16:29:01.081][eevit.py]: ViT model initialized\n"
     ]
    }
   ],
   "source": [
    "from utils import get_config, get_model\n",
    "from utils.logging_utils import yellow_txt\n",
    "from utils.arg_utils import parse_config_dict\n",
    "\n",
    "config = get_config()\n",
    "\n",
    "# ViT config\n",
    "model_config = parse_config_dict(config[\"model\"].copy())\n",
    "model = get_model(model_config, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copying weights from one model to another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "ln = nn.LayerNorm(model_config.embed_depth, eps=1e-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUnexptected Keys: Total72\u001b[0m\n",
      "\u001b[33mKeys in LGVIT but not in EEVIT\u001b[0m\n",
      "transformer.layers.0.norm_mlp.norm.weight\n",
      "transformer.layers.0.norm_mlp.norm.bias\n",
      "transformer.layers.0.norm_mlp.mlp.0.weight\n",
      "transformer.layers.0.norm_mlp.mlp.0.bias\n",
      "transformer.layers.0.norm_mlp.mlp.3.weight\n",
      "transformer.layers.0.norm_mlp.mlp.3.bias\n",
      "transformer.layers.1.norm_mlp.norm.weight\n",
      "transformer.layers.1.norm_mlp.norm.bias\n",
      "transformer.layers.1.norm_mlp.mlp.0.weight\n",
      "transformer.layers.1.norm_mlp.mlp.0.bias\n",
      "transformer.layers.1.norm_mlp.mlp.3.weight\n",
      "transformer.layers.1.norm_mlp.mlp.3.bias\n",
      "transformer.layers.2.norm_mlp.norm.weight\n",
      "transformer.layers.2.norm_mlp.norm.bias\n",
      "transformer.layers.2.norm_mlp.mlp.0.weight\n",
      "transformer.layers.2.norm_mlp.mlp.0.bias\n",
      "transformer.layers.2.norm_mlp.mlp.3.weight\n",
      "transformer.layers.2.norm_mlp.mlp.3.bias\n",
      "transformer.layers.3.norm_mlp.norm.weight\n",
      "transformer.layers.3.norm_mlp.norm.bias\n",
      "transformer.layers.3.norm_mlp.mlp.0.weight\n",
      "transformer.layers.3.norm_mlp.mlp.0.bias\n",
      "transformer.layers.3.norm_mlp.mlp.3.weight\n",
      "transformer.layers.3.norm_mlp.mlp.3.bias\n",
      "transformer.layers.4.norm_mlp.norm.weight\n",
      "transformer.layers.4.norm_mlp.norm.bias\n",
      "transformer.layers.4.norm_mlp.mlp.0.weight\n",
      "transformer.layers.4.norm_mlp.mlp.0.bias\n",
      "transformer.layers.4.norm_mlp.mlp.3.weight\n",
      "transformer.layers.4.norm_mlp.mlp.3.bias\n",
      "transformer.layers.5.norm_mlp.norm.weight\n",
      "transformer.layers.5.norm_mlp.norm.bias\n",
      "transformer.layers.5.norm_mlp.mlp.0.weight\n",
      "transformer.layers.5.norm_mlp.mlp.0.bias\n",
      "transformer.layers.5.norm_mlp.mlp.3.weight\n",
      "transformer.layers.5.norm_mlp.mlp.3.bias\n",
      "transformer.layers.6.norm_mlp.norm.weight\n",
      "transformer.layers.6.norm_mlp.norm.bias\n",
      "transformer.layers.6.norm_mlp.mlp.0.weight\n",
      "transformer.layers.6.norm_mlp.mlp.0.bias\n",
      "transformer.layers.6.norm_mlp.mlp.3.weight\n",
      "transformer.layers.6.norm_mlp.mlp.3.bias\n",
      "transformer.layers.7.norm_mlp.norm.weight\n",
      "transformer.layers.7.norm_mlp.norm.bias\n",
      "transformer.layers.7.norm_mlp.mlp.0.weight\n",
      "transformer.layers.7.norm_mlp.mlp.0.bias\n",
      "transformer.layers.7.norm_mlp.mlp.3.weight\n",
      "transformer.layers.7.norm_mlp.mlp.3.bias\n",
      "transformer.layers.8.norm_mlp.norm.weight\n",
      "transformer.layers.8.norm_mlp.norm.bias\n",
      "transformer.layers.8.norm_mlp.mlp.0.weight\n",
      "transformer.layers.8.norm_mlp.mlp.0.bias\n",
      "transformer.layers.8.norm_mlp.mlp.3.weight\n",
      "transformer.layers.8.norm_mlp.mlp.3.bias\n",
      "transformer.layers.9.norm_mlp.norm.weight\n",
      "transformer.layers.9.norm_mlp.norm.bias\n",
      "transformer.layers.9.norm_mlp.mlp.0.weight\n",
      "transformer.layers.9.norm_mlp.mlp.0.bias\n",
      "transformer.layers.9.norm_mlp.mlp.3.weight\n",
      "transformer.layers.9.norm_mlp.mlp.3.bias\n",
      "transformer.layers.10.norm_mlp.norm.weight\n",
      "transformer.layers.10.norm_mlp.norm.bias\n",
      "transformer.layers.10.norm_mlp.mlp.0.weight\n",
      "transformer.layers.10.norm_mlp.mlp.0.bias\n",
      "transformer.layers.10.norm_mlp.mlp.3.weight\n",
      "transformer.layers.10.norm_mlp.mlp.3.bias\n",
      "transformer.layers.11.norm_mlp.norm.weight\n",
      "transformer.layers.11.norm_mlp.norm.bias\n",
      "transformer.layers.11.norm_mlp.mlp.0.weight\n",
      "transformer.layers.11.norm_mlp.mlp.0.bias\n",
      "transformer.layers.11.norm_mlp.mlp.3.weight\n",
      "transformer.layers.11.norm_mlp.mlp.3.bias\n",
      "\u001b[33mMissing Keys: Total: 160\u001b[0m\n",
      "\u001b[33mKeys in EEVIT but not in LGVIT\u001b[0m\n",
      "transformer.layers.0.norm_2.weight\n",
      "transformer.layers.0.norm_2.bias\n",
      "transformer.layers.0.mlps.mlp_intermediate.0.weight\n",
      "transformer.layers.0.mlps.mlp_intermediate.0.bias\n",
      "transformer.layers.0.mlps.mlp_output.0.weight\n",
      "transformer.layers.0.mlps.mlp_output.0.bias\n",
      "transformer.layers.1.norm_2.weight\n",
      "transformer.layers.1.norm_2.bias\n",
      "transformer.layers.1.mlps.mlp_intermediate.0.weight\n",
      "transformer.layers.1.mlps.mlp_intermediate.0.bias\n",
      "transformer.layers.1.mlps.mlp_output.0.weight\n",
      "transformer.layers.1.mlps.mlp_output.0.bias\n",
      "transformer.layers.2.norm_2.weight\n",
      "transformer.layers.2.norm_2.bias\n",
      "transformer.layers.2.mlps.mlp_intermediate.0.weight\n",
      "transformer.layers.2.mlps.mlp_intermediate.0.bias\n",
      "transformer.layers.2.mlps.mlp_output.0.weight\n",
      "transformer.layers.2.mlps.mlp_output.0.bias\n",
      "transformer.layers.3.norm_2.weight\n",
      "transformer.layers.3.norm_2.bias\n",
      "transformer.layers.3.mlps.mlp_intermediate.0.weight\n",
      "transformer.layers.3.mlps.mlp_intermediate.0.bias\n",
      "transformer.layers.3.mlps.mlp_output.0.weight\n",
      "transformer.layers.3.mlps.mlp_output.0.bias\n",
      "transformer.layers.4.norm_2.weight\n",
      "transformer.layers.4.norm_2.bias\n",
      "transformer.layers.4.mlps.mlp_intermediate.0.weight\n",
      "transformer.layers.4.mlps.mlp_intermediate.0.bias\n",
      "transformer.layers.4.mlps.mlp_output.0.weight\n",
      "transformer.layers.4.mlps.mlp_output.0.bias\n",
      "transformer.layers.4.highway.highway_head.conv1.0.weight\n",
      "transformer.layers.4.highway.highway_head.conv1.0.bias\n",
      "transformer.layers.4.highway.highway_head.conv1.2.weight\n",
      "transformer.layers.4.highway.highway_head.conv1.2.bias\n",
      "transformer.layers.4.highway.highway_head.conv1.2.running_mean\n",
      "transformer.layers.4.highway.highway_head.conv1.2.running_var\n",
      "transformer.layers.4.highway.highway_head.proj.weight\n",
      "transformer.layers.4.highway.highway_head.proj.bias\n",
      "transformer.layers.4.highway.highway_head.proj_bn.weight\n",
      "transformer.layers.4.highway.highway_head.proj_bn.bias\n",
      "transformer.layers.4.highway.highway_head.proj_bn.running_mean\n",
      "transformer.layers.4.highway.highway_head.proj_bn.running_var\n",
      "transformer.layers.4.highway.highway_head.conv2.0.weight\n",
      "transformer.layers.4.highway.highway_head.conv2.0.bias\n",
      "transformer.layers.4.highway.highway_head.conv2.1.weight\n",
      "transformer.layers.4.highway.highway_head.conv2.1.bias\n",
      "transformer.layers.4.highway.highway_head.conv2.1.running_mean\n",
      "transformer.layers.4.highway.highway_head.conv2.1.running_var\n",
      "transformer.layers.4.highway.classifier.classifier.weight\n",
      "transformer.layers.4.highway.classifier.classifier.bias\n",
      "transformer.layers.5.norm_2.weight\n",
      "transformer.layers.5.norm_2.bias\n",
      "transformer.layers.5.mlps.mlp_intermediate.0.weight\n",
      "transformer.layers.5.mlps.mlp_intermediate.0.bias\n",
      "transformer.layers.5.mlps.mlp_output.0.weight\n",
      "transformer.layers.5.mlps.mlp_output.0.bias\n",
      "transformer.layers.5.highway.highway_head.conv1.0.weight\n",
      "transformer.layers.5.highway.highway_head.conv1.0.bias\n",
      "transformer.layers.5.highway.highway_head.conv1.2.weight\n",
      "transformer.layers.5.highway.highway_head.conv1.2.bias\n",
      "transformer.layers.5.highway.highway_head.conv1.2.running_mean\n",
      "transformer.layers.5.highway.highway_head.conv1.2.running_var\n",
      "transformer.layers.5.highway.highway_head.proj.weight\n",
      "transformer.layers.5.highway.highway_head.proj.bias\n",
      "transformer.layers.5.highway.highway_head.proj_bn.weight\n",
      "transformer.layers.5.highway.highway_head.proj_bn.bias\n",
      "transformer.layers.5.highway.highway_head.proj_bn.running_mean\n",
      "transformer.layers.5.highway.highway_head.proj_bn.running_var\n",
      "transformer.layers.5.highway.highway_head.conv2.0.weight\n",
      "transformer.layers.5.highway.highway_head.conv2.0.bias\n",
      "transformer.layers.5.highway.highway_head.conv2.1.weight\n",
      "transformer.layers.5.highway.highway_head.conv2.1.bias\n",
      "transformer.layers.5.highway.highway_head.conv2.1.running_mean\n",
      "transformer.layers.5.highway.highway_head.conv2.1.running_var\n",
      "transformer.layers.5.highway.classifier.classifier.weight\n",
      "transformer.layers.5.highway.classifier.classifier.bias\n",
      "transformer.layers.6.norm_2.weight\n",
      "transformer.layers.6.norm_2.bias\n",
      "transformer.layers.6.mlps.mlp_intermediate.0.weight\n",
      "transformer.layers.6.mlps.mlp_intermediate.0.bias\n",
      "transformer.layers.6.mlps.mlp_output.0.weight\n",
      "transformer.layers.6.mlps.mlp_output.0.bias\n",
      "transformer.layers.6.highway.highway_head.conv1.0.weight\n",
      "transformer.layers.6.highway.highway_head.conv1.0.bias\n",
      "transformer.layers.6.highway.highway_head.conv1.2.weight\n",
      "transformer.layers.6.highway.highway_head.conv1.2.bias\n",
      "transformer.layers.6.highway.highway_head.conv1.2.running_mean\n",
      "transformer.layers.6.highway.highway_head.conv1.2.running_var\n",
      "transformer.layers.6.highway.highway_head.conv2.0.weight\n",
      "transformer.layers.6.highway.highway_head.conv2.0.bias\n",
      "transformer.layers.6.highway.highway_head.conv2.1.weight\n",
      "transformer.layers.6.highway.highway_head.conv2.1.bias\n",
      "transformer.layers.6.highway.highway_head.conv2.1.running_mean\n",
      "transformer.layers.6.highway.highway_head.conv2.1.running_var\n",
      "transformer.layers.6.highway.classifier.classifier.weight\n",
      "transformer.layers.6.highway.classifier.classifier.bias\n",
      "transformer.layers.7.norm_2.weight\n",
      "transformer.layers.7.norm_2.bias\n",
      "transformer.layers.7.mlps.mlp_intermediate.0.weight\n",
      "transformer.layers.7.mlps.mlp_intermediate.0.bias\n",
      "transformer.layers.7.mlps.mlp_output.0.weight\n",
      "transformer.layers.7.mlps.mlp_output.0.bias\n",
      "transformer.layers.7.highway.highway_head.conv1.0.weight\n",
      "transformer.layers.7.highway.highway_head.conv1.0.bias\n",
      "transformer.layers.7.highway.highway_head.conv1.2.weight\n",
      "transformer.layers.7.highway.highway_head.conv1.2.bias\n",
      "transformer.layers.7.highway.highway_head.conv1.2.running_mean\n",
      "transformer.layers.7.highway.highway_head.conv1.2.running_var\n",
      "transformer.layers.7.highway.highway_head.conv2.0.weight\n",
      "transformer.layers.7.highway.highway_head.conv2.0.bias\n",
      "transformer.layers.7.highway.highway_head.conv2.1.weight\n",
      "transformer.layers.7.highway.highway_head.conv2.1.bias\n",
      "transformer.layers.7.highway.highway_head.conv2.1.running_mean\n",
      "transformer.layers.7.highway.highway_head.conv2.1.running_var\n",
      "transformer.layers.7.highway.classifier.classifier.weight\n",
      "transformer.layers.7.highway.classifier.classifier.bias\n",
      "transformer.layers.8.norm_2.weight\n",
      "transformer.layers.8.norm_2.bias\n",
      "transformer.layers.8.mlps.mlp_intermediate.0.weight\n",
      "transformer.layers.8.mlps.mlp_intermediate.0.bias\n",
      "transformer.layers.8.mlps.mlp_output.0.weight\n",
      "transformer.layers.8.mlps.mlp_output.0.bias\n",
      "transformer.layers.8.highway.highway_head.qkv.weight\n",
      "transformer.layers.8.highway.highway_head.proj.weight\n",
      "transformer.layers.8.highway.highway_head.proj.bias\n",
      "transformer.layers.8.highway.classifier.classifier.weight\n",
      "transformer.layers.8.highway.classifier.classifier.bias\n",
      "transformer.layers.9.norm_2.weight\n",
      "transformer.layers.9.norm_2.bias\n",
      "transformer.layers.9.mlps.mlp_intermediate.0.weight\n",
      "transformer.layers.9.mlps.mlp_intermediate.0.bias\n",
      "transformer.layers.9.mlps.mlp_output.0.weight\n",
      "transformer.layers.9.mlps.mlp_output.0.bias\n",
      "transformer.layers.9.highway.highway_head.qkv.weight\n",
      "transformer.layers.9.highway.highway_head.proj.weight\n",
      "transformer.layers.9.highway.highway_head.proj.bias\n",
      "transformer.layers.9.highway.classifier.classifier.weight\n",
      "transformer.layers.9.highway.classifier.classifier.bias\n",
      "transformer.layers.10.norm_2.weight\n",
      "transformer.layers.10.norm_2.bias\n",
      "transformer.layers.10.mlps.mlp_intermediate.0.weight\n",
      "transformer.layers.10.mlps.mlp_intermediate.0.bias\n",
      "transformer.layers.10.mlps.mlp_output.0.weight\n",
      "transformer.layers.10.mlps.mlp_output.0.bias\n",
      "transformer.layers.10.highway.highway_head.qkv.weight\n",
      "transformer.layers.10.highway.highway_head.proj.weight\n",
      "transformer.layers.10.highway.highway_head.proj.bias\n",
      "transformer.layers.10.highway.classifier.classifier.weight\n",
      "transformer.layers.10.highway.classifier.classifier.bias\n",
      "transformer.layers.11.norm_2.weight\n",
      "transformer.layers.11.norm_2.bias\n",
      "transformer.layers.11.mlps.mlp_intermediate.0.weight\n",
      "transformer.layers.11.mlps.mlp_intermediate.0.bias\n",
      "transformer.layers.11.mlps.mlp_output.0.weight\n",
      "transformer.layers.11.mlps.mlp_output.0.bias\n",
      "transformer.layers.11.highway.highway_head.qkv.weight\n",
      "transformer.layers.11.highway.highway_head.proj.weight\n",
      "transformer.layers.11.highway.highway_head.proj.bias\n",
      "transformer.layers.11.highway.classifier.classifier.weight\n",
      "transformer.layers.11.highway.classifier.classifier.bias\n"
     ]
    }
   ],
   "source": [
    "incompatible_keys = model.load_state_dict(lgvit_map, strict=False)\n",
    "\n",
    "print(yellow_txt(f\"Unexptected Keys: Total: {len(incompatible_keys.unexpected_keys)}\"))\n",
    "print(yellow_txt(\"Keys in LGVIT but not in EEVIT\"))\n",
    "for uk in incompatible_keys.unexpected_keys:\n",
    "    print(uk)\n",
    "\n",
    "print(yellow_txt(f\"Missing Keys: Total: {len(incompatible_keys.missing_keys)}\"))\n",
    "print(yellow_txt(\"Keys in EEVIT but not in LGVIT\"))\n",
    "for mk in incompatible_keys.missing_keys:\n",
    "    print(mk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example from Pytorch docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class SubNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SubNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.linear_layers = SubNet()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.linear_layers(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in net.state_dict():\n",
    "    print(param_tensor, \"\\t\", net.state_dict()[param_tensor].size())\n",
    "\n",
    "print()\n",
    "\n",
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eevit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
