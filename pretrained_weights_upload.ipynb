{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deit.embeddings.cls_token: torch.Size([1, 1, 768])\n",
      "deit.embeddings.position_embeddings: torch.Size([1, 197, 768])\n",
      "deit.embeddings.patch_embeddings.projection.weight: torch.Size([768, 3, 16, 16])\n",
      "deit.embeddings.patch_embeddings.projection.bias: torch.Size([768])\n",
      "deit.encoder.layer.0.attention.attention.query.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.0.attention.attention.query.bias: torch.Size([768])\n",
      "deit.encoder.layer.0.attention.attention.key.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.0.attention.attention.key.bias: torch.Size([768])\n",
      "deit.encoder.layer.0.attention.attention.value.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.0.attention.attention.value.bias: torch.Size([768])\n",
      "deit.encoder.layer.0.attention.output.dense.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.0.attention.output.dense.bias: torch.Size([768])\n",
      "deit.encoder.layer.0.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "deit.encoder.layer.0.intermediate.dense.bias: torch.Size([3072])\n",
      "deit.encoder.layer.0.output.dense.weight: torch.Size([768, 3072])\n",
      "deit.encoder.layer.0.output.dense.bias: torch.Size([768])\n",
      "deit.encoder.layer.0.layernorm_before.weight: torch.Size([768])\n",
      "deit.encoder.layer.0.layernorm_before.bias: torch.Size([768])\n",
      "deit.encoder.layer.0.layernorm_after.weight: torch.Size([768])\n",
      "deit.encoder.layer.0.layernorm_after.bias: torch.Size([768])\n",
      "deit.encoder.layer.1.attention.attention.query.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.1.attention.attention.query.bias: torch.Size([768])\n",
      "deit.encoder.layer.1.attention.attention.key.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.1.attention.attention.key.bias: torch.Size([768])\n",
      "deit.encoder.layer.1.attention.attention.value.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.1.attention.attention.value.bias: torch.Size([768])\n",
      "deit.encoder.layer.1.attention.output.dense.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.1.attention.output.dense.bias: torch.Size([768])\n",
      "deit.encoder.layer.1.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "deit.encoder.layer.1.intermediate.dense.bias: torch.Size([3072])\n",
      "deit.encoder.layer.1.output.dense.weight: torch.Size([768, 3072])\n",
      "deit.encoder.layer.1.output.dense.bias: torch.Size([768])\n",
      "deit.encoder.layer.1.layernorm_before.weight: torch.Size([768])\n",
      "deit.encoder.layer.1.layernorm_before.bias: torch.Size([768])\n",
      "deit.encoder.layer.1.layernorm_after.weight: torch.Size([768])\n",
      "deit.encoder.layer.1.layernorm_after.bias: torch.Size([768])\n",
      "deit.encoder.layer.2.attention.attention.query.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.2.attention.attention.query.bias: torch.Size([768])\n",
      "deit.encoder.layer.2.attention.attention.key.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.2.attention.attention.key.bias: torch.Size([768])\n",
      "deit.encoder.layer.2.attention.attention.value.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.2.attention.attention.value.bias: torch.Size([768])\n",
      "deit.encoder.layer.2.attention.output.dense.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.2.attention.output.dense.bias: torch.Size([768])\n",
      "deit.encoder.layer.2.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "deit.encoder.layer.2.intermediate.dense.bias: torch.Size([3072])\n",
      "deit.encoder.layer.2.output.dense.weight: torch.Size([768, 3072])\n",
      "deit.encoder.layer.2.output.dense.bias: torch.Size([768])\n",
      "deit.encoder.layer.2.layernorm_before.weight: torch.Size([768])\n",
      "deit.encoder.layer.2.layernorm_before.bias: torch.Size([768])\n",
      "deit.encoder.layer.2.layernorm_after.weight: torch.Size([768])\n",
      "deit.encoder.layer.2.layernorm_after.bias: torch.Size([768])\n",
      "deit.encoder.layer.3.attention.attention.query.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.3.attention.attention.query.bias: torch.Size([768])\n",
      "deit.encoder.layer.3.attention.attention.key.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.3.attention.attention.key.bias: torch.Size([768])\n",
      "deit.encoder.layer.3.attention.attention.value.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.3.attention.attention.value.bias: torch.Size([768])\n",
      "deit.encoder.layer.3.attention.output.dense.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.3.attention.output.dense.bias: torch.Size([768])\n",
      "deit.encoder.layer.3.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "deit.encoder.layer.3.intermediate.dense.bias: torch.Size([3072])\n",
      "deit.encoder.layer.3.output.dense.weight: torch.Size([768, 3072])\n",
      "deit.encoder.layer.3.output.dense.bias: torch.Size([768])\n",
      "deit.encoder.layer.3.layernorm_before.weight: torch.Size([768])\n",
      "deit.encoder.layer.3.layernorm_before.bias: torch.Size([768])\n",
      "deit.encoder.layer.3.layernorm_after.weight: torch.Size([768])\n",
      "deit.encoder.layer.3.layernorm_after.bias: torch.Size([768])\n",
      "deit.encoder.layer.4.attention.attention.query.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.4.attention.attention.query.bias: torch.Size([768])\n",
      "deit.encoder.layer.4.attention.attention.key.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.4.attention.attention.key.bias: torch.Size([768])\n",
      "deit.encoder.layer.4.attention.attention.value.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.4.attention.attention.value.bias: torch.Size([768])\n",
      "deit.encoder.layer.4.attention.output.dense.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.4.attention.output.dense.bias: torch.Size([768])\n",
      "deit.encoder.layer.4.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "deit.encoder.layer.4.intermediate.dense.bias: torch.Size([3072])\n",
      "deit.encoder.layer.4.output.dense.weight: torch.Size([768, 3072])\n",
      "deit.encoder.layer.4.output.dense.bias: torch.Size([768])\n",
      "deit.encoder.layer.4.layernorm_before.weight: torch.Size([768])\n",
      "deit.encoder.layer.4.layernorm_before.bias: torch.Size([768])\n",
      "deit.encoder.layer.4.layernorm_after.weight: torch.Size([768])\n",
      "deit.encoder.layer.4.layernorm_after.bias: torch.Size([768])\n",
      "deit.encoder.layer.5.attention.attention.query.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.5.attention.attention.query.bias: torch.Size([768])\n",
      "deit.encoder.layer.5.attention.attention.key.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.5.attention.attention.key.bias: torch.Size([768])\n",
      "deit.encoder.layer.5.attention.attention.value.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.5.attention.attention.value.bias: torch.Size([768])\n",
      "deit.encoder.layer.5.attention.output.dense.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.5.attention.output.dense.bias: torch.Size([768])\n",
      "deit.encoder.layer.5.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "deit.encoder.layer.5.intermediate.dense.bias: torch.Size([3072])\n",
      "deit.encoder.layer.5.output.dense.weight: torch.Size([768, 3072])\n",
      "deit.encoder.layer.5.output.dense.bias: torch.Size([768])\n",
      "deit.encoder.layer.5.layernorm_before.weight: torch.Size([768])\n",
      "deit.encoder.layer.5.layernorm_before.bias: torch.Size([768])\n",
      "deit.encoder.layer.5.layernorm_after.weight: torch.Size([768])\n",
      "deit.encoder.layer.5.layernorm_after.bias: torch.Size([768])\n",
      "deit.encoder.layer.6.attention.attention.query.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.6.attention.attention.query.bias: torch.Size([768])\n",
      "deit.encoder.layer.6.attention.attention.key.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.6.attention.attention.key.bias: torch.Size([768])\n",
      "deit.encoder.layer.6.attention.attention.value.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.6.attention.attention.value.bias: torch.Size([768])\n",
      "deit.encoder.layer.6.attention.output.dense.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.6.attention.output.dense.bias: torch.Size([768])\n",
      "deit.encoder.layer.6.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "deit.encoder.layer.6.intermediate.dense.bias: torch.Size([3072])\n",
      "deit.encoder.layer.6.output.dense.weight: torch.Size([768, 3072])\n",
      "deit.encoder.layer.6.output.dense.bias: torch.Size([768])\n",
      "deit.encoder.layer.6.layernorm_before.weight: torch.Size([768])\n",
      "deit.encoder.layer.6.layernorm_before.bias: torch.Size([768])\n",
      "deit.encoder.layer.6.layernorm_after.weight: torch.Size([768])\n",
      "deit.encoder.layer.6.layernorm_after.bias: torch.Size([768])\n",
      "deit.encoder.layer.7.attention.attention.query.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.7.attention.attention.query.bias: torch.Size([768])\n",
      "deit.encoder.layer.7.attention.attention.key.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.7.attention.attention.key.bias: torch.Size([768])\n",
      "deit.encoder.layer.7.attention.attention.value.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.7.attention.attention.value.bias: torch.Size([768])\n",
      "deit.encoder.layer.7.attention.output.dense.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.7.attention.output.dense.bias: torch.Size([768])\n",
      "deit.encoder.layer.7.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "deit.encoder.layer.7.intermediate.dense.bias: torch.Size([3072])\n",
      "deit.encoder.layer.7.output.dense.weight: torch.Size([768, 3072])\n",
      "deit.encoder.layer.7.output.dense.bias: torch.Size([768])\n",
      "deit.encoder.layer.7.layernorm_before.weight: torch.Size([768])\n",
      "deit.encoder.layer.7.layernorm_before.bias: torch.Size([768])\n",
      "deit.encoder.layer.7.layernorm_after.weight: torch.Size([768])\n",
      "deit.encoder.layer.7.layernorm_after.bias: torch.Size([768])\n",
      "deit.encoder.layer.8.attention.attention.query.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.8.attention.attention.query.bias: torch.Size([768])\n",
      "deit.encoder.layer.8.attention.attention.key.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.8.attention.attention.key.bias: torch.Size([768])\n",
      "deit.encoder.layer.8.attention.attention.value.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.8.attention.attention.value.bias: torch.Size([768])\n",
      "deit.encoder.layer.8.attention.output.dense.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.8.attention.output.dense.bias: torch.Size([768])\n",
      "deit.encoder.layer.8.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "deit.encoder.layer.8.intermediate.dense.bias: torch.Size([3072])\n",
      "deit.encoder.layer.8.output.dense.weight: torch.Size([768, 3072])\n",
      "deit.encoder.layer.8.output.dense.bias: torch.Size([768])\n",
      "deit.encoder.layer.8.layernorm_before.weight: torch.Size([768])\n",
      "deit.encoder.layer.8.layernorm_before.bias: torch.Size([768])\n",
      "deit.encoder.layer.8.layernorm_after.weight: torch.Size([768])\n",
      "deit.encoder.layer.8.layernorm_after.bias: torch.Size([768])\n",
      "deit.encoder.layer.9.attention.attention.query.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.9.attention.attention.query.bias: torch.Size([768])\n",
      "deit.encoder.layer.9.attention.attention.key.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.9.attention.attention.key.bias: torch.Size([768])\n",
      "deit.encoder.layer.9.attention.attention.value.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.9.attention.attention.value.bias: torch.Size([768])\n",
      "deit.encoder.layer.9.attention.output.dense.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.9.attention.output.dense.bias: torch.Size([768])\n",
      "deit.encoder.layer.9.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "deit.encoder.layer.9.intermediate.dense.bias: torch.Size([3072])\n",
      "deit.encoder.layer.9.output.dense.weight: torch.Size([768, 3072])\n",
      "deit.encoder.layer.9.output.dense.bias: torch.Size([768])\n",
      "deit.encoder.layer.9.layernorm_before.weight: torch.Size([768])\n",
      "deit.encoder.layer.9.layernorm_before.bias: torch.Size([768])\n",
      "deit.encoder.layer.9.layernorm_after.weight: torch.Size([768])\n",
      "deit.encoder.layer.9.layernorm_after.bias: torch.Size([768])\n",
      "deit.encoder.layer.10.attention.attention.query.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.10.attention.attention.query.bias: torch.Size([768])\n",
      "deit.encoder.layer.10.attention.attention.key.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.10.attention.attention.key.bias: torch.Size([768])\n",
      "deit.encoder.layer.10.attention.attention.value.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.10.attention.attention.value.bias: torch.Size([768])\n",
      "deit.encoder.layer.10.attention.output.dense.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.10.attention.output.dense.bias: torch.Size([768])\n",
      "deit.encoder.layer.10.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "deit.encoder.layer.10.intermediate.dense.bias: torch.Size([3072])\n",
      "deit.encoder.layer.10.output.dense.weight: torch.Size([768, 3072])\n",
      "deit.encoder.layer.10.output.dense.bias: torch.Size([768])\n",
      "deit.encoder.layer.10.layernorm_before.weight: torch.Size([768])\n",
      "deit.encoder.layer.10.layernorm_before.bias: torch.Size([768])\n",
      "deit.encoder.layer.10.layernorm_after.weight: torch.Size([768])\n",
      "deit.encoder.layer.10.layernorm_after.bias: torch.Size([768])\n",
      "deit.encoder.layer.11.attention.attention.query.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.11.attention.attention.query.bias: torch.Size([768])\n",
      "deit.encoder.layer.11.attention.attention.key.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.11.attention.attention.key.bias: torch.Size([768])\n",
      "deit.encoder.layer.11.attention.attention.value.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.11.attention.attention.value.bias: torch.Size([768])\n",
      "deit.encoder.layer.11.attention.output.dense.weight: torch.Size([768, 768])\n",
      "deit.encoder.layer.11.attention.output.dense.bias: torch.Size([768])\n",
      "deit.encoder.layer.11.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "deit.encoder.layer.11.intermediate.dense.bias: torch.Size([3072])\n",
      "deit.encoder.layer.11.output.dense.weight: torch.Size([768, 3072])\n",
      "deit.encoder.layer.11.output.dense.bias: torch.Size([768])\n",
      "deit.encoder.layer.11.layernorm_before.weight: torch.Size([768])\n",
      "deit.encoder.layer.11.layernorm_before.bias: torch.Size([768])\n",
      "deit.encoder.layer.11.layernorm_after.weight: torch.Size([768])\n",
      "deit.encoder.layer.11.layernorm_after.bias: torch.Size([768])\n",
      "deit.encoder.highway.0.mlp.conv1.0.weight: torch.Size([768, 768, 1, 1])\n",
      "deit.encoder.highway.0.mlp.conv1.0.bias: torch.Size([768])\n",
      "deit.encoder.highway.0.mlp.conv1.2.weight: torch.Size([768])\n",
      "deit.encoder.highway.0.mlp.conv1.2.bias: torch.Size([768])\n",
      "deit.encoder.highway.0.mlp.conv1.2.running_mean: torch.Size([768])\n",
      "deit.encoder.highway.0.mlp.conv1.2.running_var: torch.Size([768])\n",
      "deit.encoder.highway.0.mlp.conv1.2.num_batches_tracked: torch.Size([])\n",
      "deit.encoder.highway.0.mlp.proj.weight: torch.Size([768, 1, 3, 3])\n",
      "deit.encoder.highway.0.mlp.proj.bias: torch.Size([768])\n",
      "deit.encoder.highway.0.mlp.proj_bn.weight: torch.Size([768])\n",
      "deit.encoder.highway.0.mlp.proj_bn.bias: torch.Size([768])\n",
      "deit.encoder.highway.0.mlp.proj_bn.running_mean: torch.Size([768])\n",
      "deit.encoder.highway.0.mlp.proj_bn.running_var: torch.Size([768])\n",
      "deit.encoder.highway.0.mlp.proj_bn.num_batches_tracked: torch.Size([])\n",
      "deit.encoder.highway.0.mlp.conv2.0.weight: torch.Size([768, 768, 1, 1])\n",
      "deit.encoder.highway.0.mlp.conv2.0.bias: torch.Size([768])\n",
      "deit.encoder.highway.0.mlp.conv2.1.weight: torch.Size([768])\n",
      "deit.encoder.highway.0.mlp.conv2.1.bias: torch.Size([768])\n",
      "deit.encoder.highway.0.mlp.conv2.1.running_mean: torch.Size([768])\n",
      "deit.encoder.highway.0.mlp.conv2.1.running_var: torch.Size([768])\n",
      "deit.encoder.highway.0.mlp.conv2.1.num_batches_tracked: torch.Size([])\n",
      "deit.encoder.highway.0.classifier.weight: torch.Size([100, 768])\n",
      "deit.encoder.highway.0.classifier.bias: torch.Size([100])\n",
      "deit.encoder.highway.1.mlp.conv1.0.weight: torch.Size([768, 768, 1, 1])\n",
      "deit.encoder.highway.1.mlp.conv1.0.bias: torch.Size([768])\n",
      "deit.encoder.highway.1.mlp.conv1.2.weight: torch.Size([768])\n",
      "deit.encoder.highway.1.mlp.conv1.2.bias: torch.Size([768])\n",
      "deit.encoder.highway.1.mlp.conv1.2.running_mean: torch.Size([768])\n",
      "deit.encoder.highway.1.mlp.conv1.2.running_var: torch.Size([768])\n",
      "deit.encoder.highway.1.mlp.conv1.2.num_batches_tracked: torch.Size([])\n",
      "deit.encoder.highway.1.mlp.proj.weight: torch.Size([768, 1, 3, 3])\n",
      "deit.encoder.highway.1.mlp.proj.bias: torch.Size([768])\n",
      "deit.encoder.highway.1.mlp.proj_bn.weight: torch.Size([768])\n",
      "deit.encoder.highway.1.mlp.proj_bn.bias: torch.Size([768])\n",
      "deit.encoder.highway.1.mlp.proj_bn.running_mean: torch.Size([768])\n",
      "deit.encoder.highway.1.mlp.proj_bn.running_var: torch.Size([768])\n",
      "deit.encoder.highway.1.mlp.proj_bn.num_batches_tracked: torch.Size([])\n",
      "deit.encoder.highway.1.mlp.conv2.0.weight: torch.Size([768, 768, 1, 1])\n",
      "deit.encoder.highway.1.mlp.conv2.0.bias: torch.Size([768])\n",
      "deit.encoder.highway.1.mlp.conv2.1.weight: torch.Size([768])\n",
      "deit.encoder.highway.1.mlp.conv2.1.bias: torch.Size([768])\n",
      "deit.encoder.highway.1.mlp.conv2.1.running_mean: torch.Size([768])\n",
      "deit.encoder.highway.1.mlp.conv2.1.running_var: torch.Size([768])\n",
      "deit.encoder.highway.1.mlp.conv2.1.num_batches_tracked: torch.Size([])\n",
      "deit.encoder.highway.1.classifier.weight: torch.Size([100, 768])\n",
      "deit.encoder.highway.1.classifier.bias: torch.Size([100])\n",
      "deit.encoder.highway.2.mlp.conv1.0.weight: torch.Size([768, 768, 1, 1])\n",
      "deit.encoder.highway.2.mlp.conv1.0.bias: torch.Size([768])\n",
      "deit.encoder.highway.2.mlp.conv1.2.weight: torch.Size([768])\n",
      "deit.encoder.highway.2.mlp.conv1.2.bias: torch.Size([768])\n",
      "deit.encoder.highway.2.mlp.conv1.2.running_mean: torch.Size([768])\n",
      "deit.encoder.highway.2.mlp.conv1.2.running_var: torch.Size([768])\n",
      "deit.encoder.highway.2.mlp.conv1.2.num_batches_tracked: torch.Size([])\n",
      "deit.encoder.highway.2.mlp.conv2.0.weight: torch.Size([768, 768, 1, 1])\n",
      "deit.encoder.highway.2.mlp.conv2.0.bias: torch.Size([768])\n",
      "deit.encoder.highway.2.mlp.conv2.1.weight: torch.Size([768])\n",
      "deit.encoder.highway.2.mlp.conv2.1.bias: torch.Size([768])\n",
      "deit.encoder.highway.2.mlp.conv2.1.running_mean: torch.Size([768])\n",
      "deit.encoder.highway.2.mlp.conv2.1.running_var: torch.Size([768])\n",
      "deit.encoder.highway.2.mlp.conv2.1.num_batches_tracked: torch.Size([])\n",
      "deit.encoder.highway.2.classifier.weight: torch.Size([100, 768])\n",
      "deit.encoder.highway.2.classifier.bias: torch.Size([100])\n",
      "deit.encoder.highway.3.mlp.conv1.0.weight: torch.Size([768, 768, 1, 1])\n",
      "deit.encoder.highway.3.mlp.conv1.0.bias: torch.Size([768])\n",
      "deit.encoder.highway.3.mlp.conv1.2.weight: torch.Size([768])\n",
      "deit.encoder.highway.3.mlp.conv1.2.bias: torch.Size([768])\n",
      "deit.encoder.highway.3.mlp.conv1.2.running_mean: torch.Size([768])\n",
      "deit.encoder.highway.3.mlp.conv1.2.running_var: torch.Size([768])\n",
      "deit.encoder.highway.3.mlp.conv1.2.num_batches_tracked: torch.Size([])\n",
      "deit.encoder.highway.3.mlp.conv2.0.weight: torch.Size([768, 768, 1, 1])\n",
      "deit.encoder.highway.3.mlp.conv2.0.bias: torch.Size([768])\n",
      "deit.encoder.highway.3.mlp.conv2.1.weight: torch.Size([768])\n",
      "deit.encoder.highway.3.mlp.conv2.1.bias: torch.Size([768])\n",
      "deit.encoder.highway.3.mlp.conv2.1.running_mean: torch.Size([768])\n",
      "deit.encoder.highway.3.mlp.conv2.1.running_var: torch.Size([768])\n",
      "deit.encoder.highway.3.mlp.conv2.1.num_batches_tracked: torch.Size([])\n",
      "deit.encoder.highway.3.classifier.weight: torch.Size([100, 768])\n",
      "deit.encoder.highway.3.classifier.bias: torch.Size([100])\n",
      "deit.encoder.highway.4.mlp.qkv.weight: torch.Size([2304, 768])\n",
      "deit.encoder.highway.4.mlp.proj.weight: torch.Size([768, 768])\n",
      "deit.encoder.highway.4.mlp.proj.bias: torch.Size([768])\n",
      "deit.encoder.highway.4.classifier.weight: torch.Size([100, 768])\n",
      "deit.encoder.highway.4.classifier.bias: torch.Size([100])\n",
      "deit.encoder.highway.5.mlp.qkv.weight: torch.Size([2304, 768])\n",
      "deit.encoder.highway.5.mlp.proj.weight: torch.Size([768, 768])\n",
      "deit.encoder.highway.5.mlp.proj.bias: torch.Size([768])\n",
      "deit.encoder.highway.5.classifier.weight: torch.Size([100, 768])\n",
      "deit.encoder.highway.5.classifier.bias: torch.Size([100])\n",
      "deit.encoder.highway.6.mlp.qkv.weight: torch.Size([2304, 768])\n",
      "deit.encoder.highway.6.mlp.proj.weight: torch.Size([768, 768])\n",
      "deit.encoder.highway.6.mlp.proj.bias: torch.Size([768])\n",
      "deit.encoder.highway.6.classifier.weight: torch.Size([100, 768])\n",
      "deit.encoder.highway.6.classifier.bias: torch.Size([100])\n",
      "deit.encoder.highway.7.mlp.qkv.weight: torch.Size([2304, 768])\n",
      "deit.encoder.highway.7.mlp.proj.weight: torch.Size([768, 768])\n",
      "deit.encoder.highway.7.mlp.proj.bias: torch.Size([768])\n",
      "deit.encoder.highway.7.classifier.weight: torch.Size([100, 768])\n",
      "deit.encoder.highway.7.classifier.bias: torch.Size([100])\n",
      "deit.layernorm.weight: torch.Size([768])\n",
      "deit.layernorm.bias: torch.Size([768])\n",
      "deit.pooler.dense.weight: torch.Size([768, 768])\n",
      "deit.pooler.dense.bias: torch.Size([768])\n",
      "classifier.weight: torch.Size([100, 768])\n",
      "classifier.bias: torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# Function to upload and load the model\n",
    "def upload_model(model_path: str):\n",
    "    state_dict = torch.load(model_path, map_location=\"cpu\", weights_only=True)\n",
    "\n",
    "    return state_dict\n",
    "\n",
    "\n",
    "# Load the model\n",
    "model_path = \"/home/iony/DTU/f24/thesis/code/lgvit/LGViT-ViT-Cifar100/pytorch_model.bin\"\n",
    "saved_model_state_dict = upload_model(model_path)\n",
    "\n",
    "# Display the model structure\n",
    "for key, value in saved_model_state_dict.items():\n",
    "    print(f\"{key}: {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO][ViT][13:12:50.662]: Initializing Vit model...\n",
      "[INFO][ViT][13:12:50.663]: image_height: 256, image_width: 256\n",
      "[INFO][ViT][13:12:50.663]: patch_height: 32, patch_width: 32\n",
      "[INFO][ViT][13:12:50.750]: ViT model initialized\n"
     ]
    }
   ],
   "source": [
    "import utils as my_utils\n",
    "\n",
    "args = my_utils.parse_config(from_argparse=False)\n",
    "\n",
    "# Dataset config\n",
    "dataset_config = args[\"dataset\"]\n",
    "# ViT config\n",
    "model_config = args[\"model\"]\n",
    "\n",
    "model = my_utils.get_model(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch_embedding.pos_embedding: \t torch.Size([1, 65, 768])\n",
      "patch_embedding.cls_token: \t torch.Size([1, 1, 768])\n",
      "patch_embedding.patch_embedding_layers.1.weight: \t torch.Size([3072])\n",
      "patch_embedding.patch_embedding_layers.1.bias: \t torch.Size([3072])\n",
      "patch_embedding.patch_embedding_layers.2.weight: \t torch.Size([768, 3072])\n",
      "patch_embedding.patch_embedding_layers.2.bias: \t torch.Size([768])\n",
      "patch_embedding.patch_embedding_layers.3.weight: \t torch.Size([768])\n",
      "patch_embedding.patch_embedding_layers.3.bias: \t torch.Size([768])\n",
      "transformer.norm.weight: \t torch.Size([768])\n",
      "transformer.norm.bias: \t torch.Size([768])\n",
      "transformer.layers.0.norm.weight: \t torch.Size([768])\n",
      "transformer.layers.0.norm.bias: \t torch.Size([768])\n",
      "transformer.layers.0.to_qkv.weight: \t torch.Size([3072, 768])\n",
      "transformer.layers.0.to_ff.0.weight: \t torch.Size([768, 1024])\n",
      "transformer.layers.0.to_ff.0.bias: \t torch.Size([768])\n",
      "transformer.layers.0.feed_forward.net.0.weight: \t torch.Size([768])\n",
      "transformer.layers.0.feed_forward.net.0.bias: \t torch.Size([768])\n",
      "transformer.layers.0.feed_forward.net.1.weight: \t torch.Size([204, 768])\n",
      "transformer.layers.0.feed_forward.net.1.bias: \t torch.Size([204])\n",
      "transformer.layers.0.feed_forward.net.4.weight: \t torch.Size([768, 204])\n",
      "transformer.layers.0.feed_forward.net.4.bias: \t torch.Size([768])\n",
      "transformer.layers.1.norm.weight: \t torch.Size([768])\n",
      "transformer.layers.1.norm.bias: \t torch.Size([768])\n",
      "transformer.layers.1.to_qkv.weight: \t torch.Size([3072, 768])\n",
      "transformer.layers.1.to_ff.0.weight: \t torch.Size([768, 1024])\n",
      "transformer.layers.1.to_ff.0.bias: \t torch.Size([768])\n",
      "transformer.layers.1.feed_forward.net.0.weight: \t torch.Size([768])\n",
      "transformer.layers.1.feed_forward.net.0.bias: \t torch.Size([768])\n",
      "transformer.layers.1.feed_forward.net.1.weight: \t torch.Size([204, 768])\n",
      "transformer.layers.1.feed_forward.net.1.bias: \t torch.Size([204])\n",
      "transformer.layers.1.feed_forward.net.4.weight: \t torch.Size([768, 204])\n",
      "transformer.layers.1.feed_forward.net.4.bias: \t torch.Size([768])\n",
      "mlp_head.weight: \t torch.Size([100, 768])\n",
      "mlp_head.bias: \t torch.Size([100])\n",
      "last_classifier.weight: \t torch.Size([100, 768])\n",
      "last_classifier.bias: \t torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "for key, value in model.state_dict().items():\n",
    "    print(f\"{key}:\", \"\\t\", value.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0x7f6958581480\n",
      "0x7f69581fc340\n"
     ]
    }
   ],
   "source": [
    "lgvit_map = {\n",
    "    \"patch_embedding.weight\": saved_model_state_dict[\"patch_embedding.weight\"],\n",
    "    \"last_classifier.bias\": saved_model_state_dict[\"classifier.bias\"],\n",
    "    \"last_classifier.weight\": saved_model_state_dict[\"classifier.weight\"],\n",
    "}\n",
    "model.load_state_dict(lgvit_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (linear_layers): SubNet(\n",
      "    (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "    (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class SubNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SubNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.linear_layers = SubNet()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.linear_layers(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "conv1.weight \t torch.Size([6, 3, 5, 5])\n",
      "conv1.bias \t torch.Size([6])\n",
      "conv2.weight \t torch.Size([16, 6, 5, 5])\n",
      "conv2.bias \t torch.Size([16])\n",
      "linear_layers.fc1.weight \t torch.Size([120, 400])\n",
      "linear_layers.fc1.bias \t torch.Size([120])\n",
      "linear_layers.fc2.weight \t torch.Size([84, 120])\n",
      "linear_layers.fc2.bias \t torch.Size([84])\n",
      "linear_layers.fc3.weight \t torch.Size([10, 84])\n",
      "linear_layers.fc3.bias \t torch.Size([10])\n",
      "\n",
      "Optimizer's state_dict:\n",
      "state \t {}\n",
      "param_groups \t [{'lr': 0.001, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]}]\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in net.state_dict():\n",
    "    print(param_tensor, \"\\t\", net.state_dict()[param_tensor].size())\n",
    "\n",
    "print()\n",
    "\n",
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eevit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
