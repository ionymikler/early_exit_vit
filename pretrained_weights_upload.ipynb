{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactored version of weight uploading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from utils.logging_utils import yellow_txt\n",
    "from utils.arg_utils import parse_config_dict, get_config_dict\n",
    "from utils.model_utils import (\n",
    "    _create_base_architecture_mapping,\n",
    "    _create_highway_mapping,\n",
    "    _make_weight_mapping,\n",
    "    _print_incompatible_keys,\n",
    "    get_model,\n",
    ")\n",
    "\n",
    "\n",
    "# Constants for file paths\n",
    "PT_WEIGHTS_PATH = (\n",
    "    \"/home/iony/DTU/f24/thesis/code/lgvit/LGViT-ViT-Cifar100/pytorch_model.bin\"\n",
    ")\n",
    "PT_CONFIG_PATH = \"/home/iony/DTU/f24/thesis/code/lgvit/LGViT-ViT-Cifar100/config.json\"\n",
    "\n",
    "\n",
    "# Function to load the model\n",
    "def load_model(model_path: str):\n",
    "    state_dict = torch.load(model_path, map_location=\"cpu\", weights_only=True)\n",
    "    return state_dict\n",
    "\n",
    "\n",
    "def load_config_trained(config_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    loads the json config file for the trained model\n",
    "    \"\"\"\n",
    "    with open(config_path, \"r\") as f:\n",
    "        config = json.load(f)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mLGVIT structure\u001b[0m\n",
      "deit.encoder.layer.0.layernorm_after.weight: torch.Size([768])\n",
      "deit.encoder.layer.0.layernorm_after.bias: torch.Size([768])\n",
      "deit.encoder.layer.1.layernorm_after.weight: torch.Size([768])\n",
      "deit.encoder.layer.1.layernorm_after.bias: torch.Size([768])\n",
      "deit.encoder.layer.2.layernorm_after.weight: torch.Size([768])\n",
      "deit.encoder.layer.2.layernorm_after.bias: torch.Size([768])\n",
      "deit.encoder.layer.3.layernorm_after.weight: torch.Size([768])\n",
      "deit.encoder.layer.3.layernorm_after.bias: torch.Size([768])\n",
      "deit.encoder.layer.4.layernorm_after.weight: torch.Size([768])\n",
      "deit.encoder.layer.4.layernorm_after.bias: torch.Size([768])\n",
      "deit.encoder.layer.5.layernorm_after.weight: torch.Size([768])\n",
      "deit.encoder.layer.5.layernorm_after.bias: torch.Size([768])\n",
      "deit.encoder.layer.6.layernorm_after.weight: torch.Size([768])\n",
      "deit.encoder.layer.6.layernorm_after.bias: torch.Size([768])\n",
      "deit.encoder.layer.7.layernorm_after.weight: torch.Size([768])\n",
      "deit.encoder.layer.7.layernorm_after.bias: torch.Size([768])\n",
      "deit.encoder.layer.8.layernorm_after.weight: torch.Size([768])\n",
      "deit.encoder.layer.8.layernorm_after.bias: torch.Size([768])\n",
      "deit.encoder.layer.9.layernorm_after.weight: torch.Size([768])\n",
      "deit.encoder.layer.9.layernorm_after.bias: torch.Size([768])\n",
      "deit.encoder.layer.10.layernorm_after.weight: torch.Size([768])\n",
      "deit.encoder.layer.10.layernorm_after.bias: torch.Size([768])\n",
      "deit.encoder.layer.11.layernorm_after.weight: torch.Size([768])\n",
      "deit.encoder.layer.11.layernorm_after.bias: torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "# Load the model and config\n",
    "saved_model_state_dict: dict = load_model(\n",
    "    \"/home/iony/DTU/f24/thesis/code/lgvit/LGViT-ViT-Cifar100/pytorch_model.bin\"\n",
    ")\n",
    "config_pretrained = load_config_trained(\n",
    "    \"/home/iony/DTU/f24/thesis/code/lgvit/LGViT-ViT-Cifar100/config.json\"\n",
    ")\n",
    "\n",
    "# Display the LGVIT model structure\n",
    "print(yellow_txt(\"LGVIT structure\"))\n",
    "str_match = \"layernorm_after\"  # \"highway\" || \"transformer\"\n",
    "for key, value in saved_model_state_dict.items():\n",
    "    if str_match in key:\n",
    "        print(f\"{key}: {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [11:46:04.143][eevit.py]: Initializing Vit model...\n",
      "[INFO] [11:46:04.144][vit_classes.py]: Initializing PatchEmbeddings...\n",
      "[INFO] [11:46:04.154][vit_classes.py]: PatchEmbedding initialized with 197 patches (including the cls token)\n",
      "[INFO] [11:46:04.436][vit_classes.py]: Highway of type 'conv1_1({})' appended to location '4'\n",
      "[INFO] [11:46:04.525][vit_classes.py]: Highway of type 'conv1_1({})' appended to location '5'\n",
      "[INFO] [11:46:04.606][vit_classes.py]: Highway of type 'conv2_1({})' appended to location '6'\n",
      "[INFO] [11:46:04.686][vit_classes.py]: Highway of type 'conv2_1({})' appended to location '7'\n",
      "[INFO] [11:46:04.779][vit_classes.py]: Highway of type 'attention({'sr_ratio': 2})' appended to location '8'\n",
      "[INFO] [11:46:04.872][vit_classes.py]: Highway of type 'attention({'sr_ratio': 2})' appended to location '9'\n",
      "[INFO] [11:46:04.969][vit_classes.py]: Highway of type 'attention({'sr_ratio': 3})' appended to location '10'\n",
      "[INFO] [11:46:05.057][vit_classes.py]: Highway of type 'attention({'sr_ratio': 3})' appended to location '11'\n",
      "[INFO] [11:46:05.133][vit_classes.py]: TransformerEnconder initialized with 12 layers and 8 early exits\n",
      "[INFO] [11:46:05.138][eevit.py]: ViT model initialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mEEVIT structure\u001b[0m\n",
      "transformer.layers.0.mlps.norm_2.weight\n",
      "transformer.layers.0.mlps.norm_2.bias\n",
      "transformer.layers.1.mlps.norm_2.weight\n",
      "transformer.layers.1.mlps.norm_2.bias\n",
      "transformer.layers.2.mlps.norm_2.weight\n",
      "transformer.layers.2.mlps.norm_2.bias\n",
      "transformer.layers.3.mlps.norm_2.weight\n",
      "transformer.layers.3.mlps.norm_2.bias\n",
      "transformer.layers.4.mlps.norm_2.weight\n",
      "transformer.layers.4.mlps.norm_2.bias\n",
      "transformer.layers.5.mlps.norm_2.weight\n",
      "transformer.layers.5.mlps.norm_2.bias\n",
      "transformer.layers.6.mlps.norm_2.weight\n",
      "transformer.layers.6.mlps.norm_2.bias\n",
      "transformer.layers.7.mlps.norm_2.weight\n",
      "transformer.layers.7.mlps.norm_2.bias\n",
      "transformer.layers.8.mlps.norm_2.weight\n",
      "transformer.layers.8.mlps.norm_2.bias\n",
      "transformer.layers.9.mlps.norm_2.weight\n",
      "transformer.layers.9.mlps.norm_2.bias\n",
      "transformer.layers.10.mlps.norm_2.weight\n",
      "transformer.layers.10.mlps.norm_2.bias\n",
      "transformer.layers.11.mlps.norm_2.weight\n",
      "transformer.layers.11.mlps.norm_2.bias\n"
     ]
    }
   ],
   "source": [
    "# Initialize EEVIT model\n",
    "config = get_config_dict()\n",
    "model_config = parse_config_dict(config[\"model\"].copy())\n",
    "model = get_model(model_config, verbose=True)\n",
    "\n",
    "# Display the EEVIT model structure\n",
    "print(yellow_txt(\"EEVIT structure\"))\n",
    "str_matches = (\n",
    "    \"norm_2\",\n",
    ")  # NOTE: needs to have the comma at the end if only one element\n",
    "for key, value in model.state_dict().items():\n",
    "    if all([match in key for match in str_matches]):\n",
    "        print(f\"{key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [11:46:07.280][utils.model_utils]: Unexpected Keys: 0\n",
      "[INFO] [11:46:07.280][utils.model_utils]: Unexpected Keys: 0\n",
      "[INFO] [11:46:07.281][utils.model_utils]: Missing Keys: 0\n",
      "[INFO] [11:46:07.281][utils.model_utils]: Missing Keys: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected Keys (Keys in LGVIT but not in EEVIT): Total: 0\n",
      "Missing Keys (Keys in EEVIT but not in LGVIT) Total: 0\n"
     ]
    }
   ],
   "source": [
    "# Create base architecture mapping\n",
    "keys_map = _create_base_architecture_mapping(config_pretrained)\n",
    "\n",
    "# Create highway mapping\n",
    "hw_keys = _create_highway_mapping(\n",
    "    model_config.early_exit_config.exit_list, saved_model_state_dict, config_pretrained\n",
    ")\n",
    "\n",
    "# Update keys_map with highway keys\n",
    "keys_map.update(hw_keys)\n",
    "\n",
    "# Create the weights mapping dictionary\n",
    "lgvit_map = _make_weight_mapping(saved_model_state_dict, keys_map)\n",
    "\n",
    "# Load weights into model\n",
    "incompatible_keys = model.load_state_dict(lgvit_map, strict=False)\n",
    "\n",
    "# Print results\n",
    "_print_incompatible_keys(incompatible_keys, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example from Pytorch docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class SubNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SubNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.linear_layers = SubNet()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.linear_layers(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in net.state_dict():\n",
    "    print(param_tensor, \"\\t\", net.state_dict()[param_tensor].size())\n",
    "\n",
    "print()\n",
    "\n",
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eevit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
